{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d2b425c",
   "metadata": {},
   "source": [
    "# 1. What is the purpose of text preprocessing in NLP, and why is it essential before analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f223149f",
   "metadata": {},
   "source": [
    "Text preprocessing is a critical step in NLP that involves refining and structuring raw text data1to facilitate effective analysis, interpretation, and modeling. Text preprocessing techniques include\n",
    "\n",
    "1)Lower casing: converting text data into lower case.\n",
    "\n",
    "2)Tokenization: splitting text into words, phrases, symbols, etc.\n",
    "\n",
    "3)Punctuation mark removal: removing punctuation marks from text.\n",
    "\n",
    "4)Stemming: reducing words to their root form.\n",
    "\n",
    "5)Lemmatization: reducing words to their base form using grammatical rules.\n",
    "\n",
    "6)Part-of-speech tagging: assigning grammatical categories to words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdabb71c",
   "metadata": {},
   "source": [
    "# 2. Describe tokenization in NLP and explain its significance in text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8725a41d",
   "metadata": {},
   "source": [
    "Tokenization is an essential part of natural language processing.\n",
    "\n",
    "It involves splitting a text into smaller pieces, known as tokens. These tokens can be words, phrases or even characters and are the basis for any NLP task such as sentiment analysis,  machine translation and text summarization.\n",
    "\n",
    "There are three primary approaches to tokenization: rule-based, dictionary-based, and statistical-based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eca0fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " It involves splitting a text into smaller pieces, known as tokens. These tokens can be words, phrases or even characters and are the basis for any NLP task such as sentiment analysis,  machine translation and text summarization.\n",
      "\n",
      "Aftr sentence Tokanization:\n",
      " ['It involves splitting a text into smaller pieces, known as tokens.', 'These tokens can be words, phrases or even characters and are the basis for any NLP task such as sentiment analysis,  machine translation and text summarization.']\n",
      "\n",
      "no of sentnces:\n",
      " 2\n",
      "======================================================================\n",
      "\n",
      "Original text:\n",
      " It involves splitting a text into smaller pieces, known as tokens. These tokens can be words, phrases or even characters and are the basis for any NLP task such as sentiment analysis,  machine translation and text summarization.\n",
      "\n",
      "Aftr word Tokanization:\n",
      " ['It', 'involves', 'splitting', 'a', 'text', 'into', 'smaller', 'pieces', ',', 'known', 'as', 'tokens', '.', 'These', 'tokens', 'can', 'be', 'words', ',', 'phrases', 'or', 'even', 'characters', 'and', 'are', 'the', 'basis', 'for', 'any', 'NLP', 'task', 'such', 'as', 'sentiment', 'analysis', ',', 'machine', 'translation', 'and', 'text', 'summarization', '.']\n",
      "\n",
      "no of words:\n",
      " 42\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text=\"\"\"It involves splitting a text into smaller pieces, known as tokens. These tokens can be words, phrases or even characters and are the basis for any NLP task such as sentiment analysis,  machine translation and text summarization.\"\"\"\n",
    "print('Original text:\\n',text)\n",
    "print()\n",
    "tokenised_sent=sent_tokenize(text)\n",
    "print('Aftr sentence Tokanization:\\n',tokenised_sent)\n",
    "print()\n",
    "print('no of sentnces:\\n',len(tokenised_sent))\n",
    "print('='*70)\n",
    "print()\n",
    "\n",
    "print('Original text:\\n',text)\n",
    "print()\n",
    "tokenised_word=word_tokenize(text)\n",
    "print('Aftr word Tokanization:\\n',tokenised_word)\n",
    "print()\n",
    "print('no of words:\\n',len(tokenised_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d038c03f",
   "metadata": {},
   "source": [
    "# 3. What are the differences between stemming and lemmatization in NLP? When would you choose one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5c749",
   "metadata": {},
   "source": [
    "Stemming is a faster process than lemmatization, but lemmatization has higher accuracy.\n",
    "\n",
    "Stemming is a rule-based approach, whereas lemmatization is a canonical dictionary-based approach.\n",
    "\n",
    "Stemming chops off the word irrespective of the context, whereas lemmatization is context-dependent.\n",
    "\n",
    "Lemmatization deals only with inflectional variance, whereas stemming may also deal with derivational variance.\n",
    "\n",
    "Lemmatization uses corpus for stop words and WordNet corpus to produce lemma, whereas stemming algorithms donâ€™t actually know the meaning of the word in the language it belongs to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f33f4c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Tokenized words - without stemming:\n",
      "\n",
      "\t ['It', 'involves', 'splitting', 'a', 'text', 'into', 'smaller', 'pieces', ',', 'known', 'as', 'tokens', '.', 'These', 'tokens', 'can', 'be', 'words', ',', 'phrases', 'or', 'even', 'characters', 'and', 'are', 'the', 'basis', 'for', 'any', 'NLP', 'task', 'such', 'as', 'sentiment', 'analysis', ',', 'machine', 'translation', 'and', 'text', 'summarization', '.']\n",
      "======================================================================\n",
      "\n",
      "Tokenized words - afer stemming are:\n",
      "\t ['it', 'involv', 'split', 'a', 'text', 'into', 'smaller', 'piec', ',', 'known', 'as', 'token', '.', 'these', 'token', 'can', 'be', 'word', ',', 'phrase', 'or', 'even', 'charact', 'and', 'are', 'the', 'basi', 'for', 'ani', 'nlp', 'task', 'such', 'as', 'sentiment', 'analysi', ',', 'machin', 'translat', 'and', 'text', 'summar', '.']\n",
      "======================================================================\n",
      "lemmarized words:\n",
      " ['It', 'involve', 'split', 'a', 'text', 'into', 'smaller', 'piece', ',', 'know', 'as', 'tokens', '.', 'These', 'tokens', 'can', 'be', 'word', ',', 'phrase', 'or', 'even', 'character', 'and', 'be', 'the', 'basis', 'for', 'any', 'NLP', 'task', 'such', 'as', 'sentiment', 'analysis', ',', 'machine', 'translation', 'and', 'text', 'summarization', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps=PorterStemmer()\n",
    "\n",
    "stemmed_words=[]\n",
    "\n",
    "for w in tokenised_word:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "    \n",
    "\n",
    "print('='*70)\n",
    "print('Tokenized words - without stemming:\\n\\n\\t',tokenised_word)\n",
    "print('='*70)\n",
    "print('\\nTokenized words - afer stemming are:\\n\\t',stemmed_words)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemma=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "lemma_words=[lemma.lemmatize(word,pos='v') for word in tokenised_word ]\n",
    "\n",
    "print('='*70)\n",
    "print('lemmarized words:\\n',lemma_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5858ad35",
   "metadata": {},
   "source": [
    "# 4. Explain the concept of stop words and their role in text preprocessing. How do they impact NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cec835",
   "metadata": {},
   "source": [
    "The words which are generally filtered out before processing a natural language are called stop words. These are actually the most common words in any language (like articles, prepositions, pronouns, conjunctions, etc) and does not add much information to the text.\n",
    "\n",
    "Stop words are available in abundance in any human language. By removing these words, we remove the low-level information from our text in order to give more focus to the important information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55764da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of words:\t 42\n",
      "======================================================================\n",
      "Tokenized words - with stop words:\n",
      "\n",
      "\t ['It', 'involves', 'splitting', 'a', 'text', 'into', 'smaller', 'pieces', ',', 'known', 'as', 'tokens', '.', 'These', 'tokens', 'can', 'be', 'words', ',', 'phrases', 'or', 'even', 'characters', 'and', 'are', 'the', 'basis', 'for', 'any', 'NLP', 'task', 'such', 'as', 'sentiment', 'analysis', ',', 'machine', 'translation', 'and', 'text', 'summarization', '.']\n",
      "======================================================================\n",
      "Length after the remoal of stopwords:\t 28\n",
      "======================================================================\n",
      "\n",
      "Tokenized words - afer removing the stopwords are:\n",
      "\t ['It', 'involves', 'splitting', 'text', 'smaller', 'pieces', ',', 'known', 'tokens', '.', 'These', 'tokens', 'words', ',', 'phrases', 'even', 'characters', 'basis', 'NLP', 'task', 'sentiment', 'analysis', ',', 'machine', 'translation', 'text', 'summarization', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens=[]\n",
    "for w in tokenised_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_tokens.append(w)\n",
    "print('Length of words:\\t',len(tokenised_word))\n",
    "print('='*70)\n",
    "print('Tokenized words - with stop words:\\n\\n\\t',tokenised_word)\n",
    "print('='*70)\n",
    "print('Length after the remoal of stopwords:\\t',len(filtered_tokens))\n",
    "print('='*70)\n",
    "print('\\nTokenized words - afer removing the stopwords are:\\n\\t',filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaa5f5a",
   "metadata": {},
   "source": [
    "# 5. How does the process of removing punctuation contribute to text preprocessing in NLP? What are its benefits?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd4ed8c",
   "metadata": {},
   "source": [
    "Text cleaning or Text pre-processing is a mandatory step when we are working with text in Natural Language Processing (NLP).  In real-life human writable text data contain various words with the wrong spelling, short words, special symbols, emojis, etc. we need to clean this kind of noisy text data before feeding it to the machine learning model.\n",
    "\n",
    "Many word embedding matrix support punctuation and special symbols. It that case, we need to retain punctuation as that models are aware of the difference between hurray and hurray!. Even in this scenario, the model works better with punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d226dbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " It involves splitting a text into smaller pieces, known as tokens. These tokens can be words, phrases or even characters and are the basis for any NLP task such as sentiment analysis,  machine translation and text summarization.\n",
      "======================================================================\n",
      "after removing puntuations:\n",
      "\n",
      "['It', 'involves', 'splitting', 'a', 'text', 'into', 'smaller', 'pieces', 'known', 'as', 'tokens', 'These', 'tokens', 'can', 'be', 'words', 'phrases', 'or', 'even', 'characters', 'and', 'are', 'the', 'basis', 'for', 'any', 'NLP', 'task', 'such', 'as', 'sentiment', 'analysis', 'machine', 'translation', 'and', 'text', 'summarization']\n"
     ]
    }
   ],
   "source": [
    "words=[word for word in tokenised_word if word.isalpha()]\n",
    "\n",
    "print('Original text:\\n',text)\n",
    "print('='*70)\n",
    "\n",
    "print('after removing puntuations:\\n')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51b35f",
   "metadata": {},
   "source": [
    "# 6. Discuss the importance of lowercase conversion in text preprocessing. Why is it a common step in NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d05fd3",
   "metadata": {},
   "source": [
    "Lowercasing, or converting all characters in a text to lowercase, is a common and crucial step in text preprocessing for various natural language processing (NLP) tasks.Here are several reasons why lowercase conversion is important:\n",
    "\n",
    "1)Uniform Representation.\n",
    "\n",
    "2)Consistent Tokenization.\n",
    "\n",
    "3)Normalization for Analysis.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2da2008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " It involves splitting a text into smaller pieces, known as tokens. These tokens can be words, phrases or even characters and are the basis for any NLP task such as sentiment analysis,  machine translation and text summarization.\n",
      "======================================================================\n",
      "after lowering the case:\n",
      "\n",
      "['it', 'involves', 'splitting', 'a', 'text', 'into', 'smaller', 'pieces', ',', 'known', 'as', 'tokens', '.', 'these', 'tokens', 'can', 'be', 'words', ',', 'phrases', 'or', 'even', 'characters', 'and', 'are', 'the', 'basis', 'for', 'any', 'nlp', 'task', 'such', 'as', 'sentiment', 'analysis', ',', 'machine', 'translation', 'and', 'text', 'summarization', '.']\n"
     ]
    }
   ],
   "source": [
    "lower_words=[word.lower() for word in tokenised_word]\n",
    "print('Original text:\\n',text)\n",
    "print('='*70)\n",
    "\n",
    "print('after lowering the case:\\n')\n",
    "print(lower_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce05c94f",
   "metadata": {},
   "source": [
    "# 7. Explain the term \"vectorization\" concerning text data. How does techniques like CountVectorizer contribute to text preprocessing in NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9831625",
   "metadata": {},
   "source": [
    "Vectorization in the context of text data refers to the process of converting textual information into numerical vectors that can be used as input for machine learning models.\n",
    "\n",
    "CountVectorizer converts a collection of text documents into a matrix of token counts. Each row of the matrix corresponds to a document, and each column corresponds to a unique word (or token) in the entire collection. The values in the matrix represent the frequency of each word in the respective documents.\n",
    "\n",
    "The output of CountVectorizer is typically a sparse matrix, where most of the entries are zero because not all words occur in every document. This sparse representation is memory-efficient, especially when dealing with large text corpora.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1982646d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:\n",
      " ['analysis' 'and' 'any' 'are' 'as' 'basis' 'be' 'can' 'characters' 'even'\n",
      " 'for' 'into' 'involves' 'it' 'known' 'machine' 'nlp' 'or' 'phrases'\n",
      " 'pieces' 'sentiment' 'smaller' 'splitting' 'such' 'summarization' 'task'\n",
      " 'text' 'the' 'these' 'tokens' 'translation' 'words']\n",
      "**********************************************************************\n",
      "Token counts matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vector=CountVectorizer()\n",
    "\n",
    "x=vector.fit_transform(tokenised_word)\n",
    "\n",
    "feature_names=vector.get_feature_names_out()\n",
    "\n",
    "print('Feature names:\\n',feature_names)\n",
    "print('*'*70)\n",
    "print('Token counts matrix:')\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c5d5e",
   "metadata": {},
   "source": [
    "# 8. Describe the concept of normalization in NLP. Provide examples of normalization techniques used in text preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47054fd9",
   "metadata": {},
   "source": [
    "Normalization is a crucial step in Natural Language Processing (NLP) that involves cleaning and preprocessing text data to make it consistent and usable for different NLP tasks.\n",
    "\n",
    "Normalization techniques used in text preprocessing are :\n",
    "\n",
    "1)Case Normalization\n",
    "\n",
    "2)Punctuation Removal\n",
    "\n",
    "3)Stop Word Removal\n",
    "\n",
    "4)Stemming\n",
    "\n",
    "5)Lemmatization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfcb403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
